{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISLP Chapter 2 - Question 10: Boston Housing Dataset Analysis\n",
    "\n",
    "**Group 1 - AUCA Data Mining Course**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Boston housing dataset from the ISLP library. We'll explore the dataset structure, create visualizations, and answer all parts of Question 10.\n",
    "\n",
    "## Question 10 Overview\n",
    "\n",
    "This exercise involves the Boston housing dataset and includes:\n",
    "- (a) Loading the Boston dataset from ISLP library\n",
    "- (b) Examining dataset dimensions and structure\n",
    "- (c) Creating pairwise scatterplots of predictors\n",
    "- (d) Analyzing crime rate associations\n",
    "- (e) Identifying suburbs with high values for various predictors\n",
    "- (f) Counting Charles River suburbs\n",
    "- (g) Finding median pupil-teacher ratio\n",
    "- (h) Analyzing the suburb with lowest median home value\n",
    "- (i) Examining room count patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Helper function to convert sklearn's Bunch to DataFrame (for compatibility)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(name):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoston\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\buble\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Helper function to convert sklearn's Bunch to DataFrame (for compatibility)\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "def load_data(name):\n",
    "    if name == 'Boston':\n",
    "        boston = load_boston()\n",
    "        df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "        df['medv'] = boston.target\n",
    "        # Rename columns to match ISLP naming if needed\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{name}' not supported in this notebook.\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Load the Boston Dataset\n",
    "\n",
    "Let's load the Boston housing dataset from the ISLP library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston dataset from ISLP library\n",
    "Boston = load_data('Boston')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Boston Housing Dataset loaded successfully!\")\n",
    "print(f\"Dataset type: {type(Boston)}\")\n",
    "print(f\"Dataset shape: {Boston.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "Boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Dataset Dimensions and Structure\n",
    "\n",
    "Let's examine the dimensions of the dataset and understand what the rows and columns represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=\" * 50)\n",
    "print(\"BOSTON HOUSING DATASET INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Number of rows (observations): {Boston.shape[0]}\")\n",
    "print(f\"Number of columns (variables): {Boston.shape[1]}\")\n",
    "\n",
    "print(\"\\nColumn names and data types:\")\n",
    "print(Boston.dtypes)\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "Boston.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed description of what rows and columns represent\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHAT ROWS AND COLUMNS REPRESENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ROWS: Each row represents a suburb/town in the Boston area\")\n",
    "print(\"COLUMNS: Various socioeconomic and housing characteristics\")\n",
    "\n",
    "# Variable descriptions\n",
    "variable_descriptions = {\n",
    "    'crim': 'Per capita crime rate by town',\n",
    "    'zn': 'Proportion of residential land zoned for lots over 25,000 sq.ft',\n",
    "    'indus': 'Proportion of non-retail business acres per town',\n",
    "    'chas': 'Charles River dummy variable (1 if tract bounds river; 0 otherwise)',\n",
    "    'nox': 'Nitric oxides concentration (parts per 10 million)',\n",
    "    'rm': 'Average number of rooms per dwelling',\n",
    "    'age': 'Proportion of owner-occupied units built prior to 1940',\n",
    "    'dis': 'Weighted distances to employment centres',\n",
    "    'rad': 'Index of accessibility to radial highways',\n",
    "    'tax': 'Full-value property-tax rate per $10,000',\n",
    "    'ptratio': 'Pupil-teacher ratio by town',\n",
    "    'lstat': '% lower status of the population',\n",
    "    'medv': 'Median value of owner-occupied homes in $1000s (TARGET VARIABLE)'\n",
    "}\n",
    "\n",
    "print(\"\\nVariable Descriptions:\")\n",
    "print(\"-\" * 80)\n",
    "for var, desc in variable_descriptions.items():\n",
    "    if var.upper() in Boston.columns or var.lower() in Boston.columns:\n",
    "        print(f\"{var.upper():10} : {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get some basic descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"\\nDESCRIPTIVE STATISTICS:\")\n",
    "print(\"=\" * 40)\n",
    "Boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c): Pairwise Scatterplots\n",
    "\n",
    "Let's create pairwise scatterplots of the predictors to understand relationships between variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise scatterplot matrix for first 6 variables\n",
    "# (We'll do this in subsets to make the plots readable)\n",
    "\n",
    "print(\"Creating pairwise scatterplots...\")\n",
    "\n",
    "# First subset of variables\n",
    "subset1_vars = Boston.columns[:6]\n",
    "pd.plotting.scatter_matrix(Boston[subset1_vars], \n",
    "                          figsize=(15, 12), \n",
    "                          alpha=0.6,\n",
    "                          diagonal='hist')\n",
    "plt.suptitle('Pairwise Scatterplots - Variables 1-6', fontsize=16, y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second subset of variables\n",
    "subset2_vars = Boston.columns[6:12]\n",
    "pd.plotting.scatter_matrix(Boston[subset2_vars], \n",
    "                          figsize=(15, 12), \n",
    "                          alpha=0.6,\n",
    "                          diagonal='hist')\n",
    "plt.suptitle('Pairwise Scatterplots - Variables 7-12', fontsize=16, y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix heatmap for better understanding of relationships\n",
    "plt.figure(figsize=(14, 12))\n",
    "correlation_matrix = Boston.corr()\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='RdBu_r', \n",
    "            center=0, \n",
    "            square=True, \n",
    "            fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Correlation Matrix - Boston Housing Dataset', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings from Pairwise Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's identify the strongest correlations\n",
    "print(\"FINDINGS FROM PAIRWISE SCATTERPLOTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find strongest positive and negative correlations\n",
    "# Get upper triangular part of correlation matrix (avoid duplicates)\n",
    "upper_triangle = np.triu(correlation_matrix, k=1)\n",
    "upper_triangle[upper_triangle == 0] = np.nan\n",
    "\n",
    "# Find indices of strongest correlations\n",
    "strong_corr_threshold = 0.7\n",
    "\n",
    "print(f\"\\nStrongest correlations (|r| > {strong_corr_threshold}):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > strong_corr_threshold:\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            print(f\"{var1.upper()} vs {var2.upper()}: {corr_val:.3f}\")\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"‚Ä¢ Strong positive correlation between RAD and TAX (highway access and taxes)\")\n",
    "print(\"‚Ä¢ Strong negative correlation between DIS and NOX (distance to employment and pollution)\")\n",
    "print(\"‚Ä¢ AGE and NOX are strongly correlated (older areas tend to be more polluted)\")\n",
    "print(\"‚Ä¢ Some multicollinearity exists among predictors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d): Crime Rate Associations\n",
    "\n",
    "Let's examine which predictors are associated with per capita crime rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations with crime rate\n",
    "print(\"CRIME RATE ASSOCIATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get correlations with crime rate (CRIM)\n",
    "crime_correlations = Boston.corr()['crim'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlations with CRIM (absolute values, sorted):\")\n",
    "print(\"-\" * 50)\n",
    "for var, corr in crime_correlations.items():\n",
    "    if var != 'crim':\n",
    "        direction = \"(+)\" if Boston.corr()['crim'][var] > 0 else \"(-)\"\n",
    "        print(f\"{var.upper():10} : {corr:.3f} {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the strongest crime rate relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Crime Rate vs. Most Correlated Variables', fontsize=16)\n",
    "\n",
    "# Top 4 most correlated variables (excluding crim itself)\n",
    "top_vars = crime_correlations.drop('crim').head(4).index\n",
    "\n",
    "for i, var in enumerate(top_vars):\n",
    "    row, col = i // 2, i % 2\n",
    "    axes[row, col].scatter(Boston[var], Boston['crim'], alpha=0.6)\n",
    "    axes[row, col].set_xlabel(var.upper())\n",
    "    axes[row, col].set_ylabel('CRIM (Crime Rate)')\n",
    "    \n",
    "    # Add correlation coefficient to plot\n",
    "    corr = Boston.corr()['crim'][var]\n",
    "    axes[row, col].set_title(f'Correlation: {corr:.3f}')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(Boston[var], Boston['crim'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[row, col].plot(Boston[var], p(Boston[var]), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the relationships\n",
    "print(\"\\nINTERPRETATION OF CRIME RATE RELATIONSHIPS:\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\nüî¥ STRONGEST POSITIVE ASSOCIATIONS:\")\n",
    "print(\"   ‚Ä¢ RAD (Highway Accessibility): Areas with better highway access have higher crime\")\n",
    "print(\"   ‚Ä¢ TAX (Property Tax Rate): Higher tax areas correlate with higher crime\")\n",
    "print(\"   ‚Ä¢ INDUS (Industrial Proportion): More industrial areas have higher crime rates\")\n",
    "print(\"   ‚Ä¢ NOX (Air Pollution): More polluted areas have higher crime rates\")\n",
    "\n",
    "print(\"\\nüîµ STRONGEST NEGATIVE ASSOCIATIONS:\")\n",
    "print(\"   ‚Ä¢ DIS (Distance to Employment): Areas closer to employment centers have higher crime\")\n",
    "print(\"   ‚Ä¢ MEDV (Home Values): Higher value areas have lower crime rates\")\n",
    "\n",
    "print(\"\\nüí° OVERALL PATTERN:\")\n",
    "print(\"   Urban, accessible, industrial areas with higher taxes tend to have more crime.\")\n",
    "print(\"   This suggests crime is associated with urban density and accessibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e): High Values Analysis\n",
    "\n",
    "Let's identify suburbs with particularly high crime rates, tax rates, and pupil-teacher ratios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze suburbs with particularly high values\n",
    "print(\"HIGH VALUES ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Define what constitutes \"high\" (top 5% for each variable)\n",
    "high_percentile = 0.95\n",
    "\n",
    "# Crime rate analysis\n",
    "high_crime_threshold = Boston['crim'].quantile(high_percentile)\n",
    "high_crime_suburbs = Boston[Boston['crim'] > high_crime_threshold]\n",
    "\n",
    "print(f\"\\nüö® HIGH CRIME RATE SUBURBS (top 5%):\")\n",
    "print(f\"   Threshold: {high_crime_threshold:.2f} crimes per capita\")\n",
    "print(f\"   Number of suburbs: {len(high_crime_suburbs)}\")\n",
    "print(f\"   Crime rate range: {Boston['crim'].min():.3f} to {Boston['crim'].max():.3f}\")\n",
    "print(f\"   Mean crime rate: {Boston['crim'].mean():.3f}\")\n",
    "\n",
    "# Tax rate analysis\n",
    "high_tax_threshold = Boston['tax'].quantile(high_percentile)\n",
    "high_tax_suburbs = Boston[Boston['tax'] > high_tax_threshold]\n",
    "\n",
    "print(f\"\\nüí∞ HIGH TAX RATE SUBURBS (top 5%):\")\n",
    "print(f\"   Threshold: ${high_tax_threshold:.0f} per $10,000\")\n",
    "print(f\"   Number of suburbs: {len(high_tax_suburbs)}\")\n",
    "print(f\"   Tax rate range: ${Boston['tax'].min():.0f} to ${Boston['tax'].max():.0f}\")\n",
    "print(f\"   Mean tax rate: ${Boston['tax'].mean():.0f}\")\n",
    "\n",
    "# Pupil-teacher ratio analysis\n",
    "high_ptratio_threshold = Boston['ptratio'].quantile(high_percentile)\n",
    "high_ptratio_suburbs = Boston[Boston['ptratio'] > high_ptratio_threshold]\n",
    "\n",
    "print(f\"\\nüë• HIGH PUPIL-TEACHER RATIO SUBURBS (top 5%):\")\n",
    "print(f\"   Threshold: {high_ptratio_threshold:.1f} students per teacher\")\n",
    "print(f\"   Number of suburbs: {len(high_ptratio_suburbs)}\")\n",
    "print(f\"   PT ratio range: {Boston['ptratio'].min():.1f} to {Boston['ptratio'].max():.1f}\")\n",
    "print(f\"   Mean PT ratio: {Boston['ptratio'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distributions and highlight extreme values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Distribution of Key Variables with Extreme Values Highlighted', fontsize=16)\n",
    "\n",
    "# Crime rate distribution\n",
    "axes[0].hist(Boston['crim'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0].axvline(high_crime_threshold, color='darkred', linestyle='--', linewidth=2, \n",
    "                label=f'95th percentile: {high_crime_threshold:.2f}')\n",
    "axes[0].set_xlabel('Crime Rate per Capita')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Crime Rate Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Tax rate distribution\n",
    "axes[1].hist(Boston['tax'], bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1].axvline(high_tax_threshold, color='darkgreen', linestyle='--', linewidth=2,\n",
    "                label=f'95th percentile: ${high_tax_threshold:.0f}')\n",
    "axes[1].set_xlabel('Property Tax Rate per $10,000')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Tax Rate Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "# Pupil-teacher ratio distribution\n",
    "axes[2].hist(Boston['ptratio'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[2].axvline(high_ptratio_threshold, color='darkblue', linestyle='--', linewidth=2,\n",
    "                label=f'95th percentile: {high_ptratio_threshold:.1f}')\n",
    "axes[2].set_xlabel('Pupil-Teacher Ratio')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Pupil-Teacher Ratio Distribution')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Charles River Analysis\n",
    "\n",
    "Let's find out how many suburbs bound the Charles River:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charles River analysis\n",
    "print(\"CHARLES RIVER ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Count suburbs bounding Charles River (chas = 1)\n",
    "charles_river_suburbs = Boston[Boston['chas'] == 1]\n",
    "total_suburbs = len(Boston)\n",
    "charles_count = len(charles_river_suburbs)\n",
    "charles_percentage = (charles_count / total_suburbs) * 100\n",
    "\n",
    "print(f\"\\nüèûÔ∏è  CHARLES RIVER PROXIMITY:\")\n",
    "print(f\"   Total suburbs in dataset: {total_suburbs}\")\n",
    "print(f\"   Suburbs bounding Charles River: {charles_count}\")\n",
    "print(f\"   Percentage: {charles_percentage:.1f}%\")\n",
    "\n",
    "# Compare characteristics of river vs non-river suburbs\n",
    "non_river_suburbs = Boston[Boston['chas'] == 0]\n",
    "\n",
    "print(f\"\\nüìä COMPARISON OF RIVER vs NON-RIVER SUBURBS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "comparison_vars = ['medv', 'crim', 'tax', 'ptratio', 'rm']\n",
    "for var in comparison_vars:\n",
    "    river_mean = charles_river_suburbs[var].mean()\n",
    "    non_river_mean = non_river_suburbs[var].mean()\n",
    "    print(f\"{var.upper():8} - River: {river_mean:6.2f}, Non-River: {non_river_mean:6.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Charles River impact\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Charles River Impact on Key Variables', fontsize=16)\n",
    "\n",
    "# Box plots comparing river vs non-river suburbs\n",
    "vars_to_plot = ['medv', 'crim', 'tax', 'rm']\n",
    "var_titles = ['Median Home Value ($1000s)', 'Crime Rate', 'Tax Rate', 'Average Rooms']\n",
    "\n",
    "for i, (var, title) in enumerate(zip(vars_to_plot, var_titles)):\n",
    "    row, col = i // 2, i % 2\n",
    "    \n",
    "    # Create box plot\n",
    "    data_to_plot = [non_river_suburbs[var], charles_river_suburbs[var]]\n",
    "    box_plot = axes[row, col].boxplot(data_to_plot, labels=['Non-River', 'River'], patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    for patch, color in zip(box_plot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    axes[row, col].set_title(title)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g): Median Pupil-Teacher Ratio\n",
    "\n",
    "Let's find the median pupil-teacher ratio among all towns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median pupil-teacher ratio\n",
    "print(\"PUPIL-TEACHER RATIO ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "median_ptratio = Boston['ptratio'].median()\n",
    "mean_ptratio = Boston['ptratio'].mean()\n",
    "std_ptratio = Boston['ptratio'].std()\n",
    "min_ptratio = Boston['ptratio'].min()\n",
    "max_ptratio = Boston['ptratio'].max()\n",
    "\n",
    "print(f\"\\nüë®‚Äçüè´ PUPIL-TEACHER RATIO STATISTICS:\")\n",
    "print(f\"   Median pupil-teacher ratio: {median_ptratio:.2f}\")\n",
    "print(f\"   Mean pupil-teacher ratio: {mean_ptratio:.2f}\")\n",
    "print(f\"   Standard deviation: {std_ptratio:.2f}\")\n",
    "print(f\"   Range: {min_ptratio:.1f} to {max_ptratio:.1f}\")\n",
    "\n",
    "# Educational context\n",
    "print(f\"\\nüìö EDUCATIONAL CONTEXT:\")\n",
    "if median_ptratio > 18:\n",
    "    print(f\"   The median ratio of {median_ptratio:.1f} suggests potential overcrowding in schools.\")\n",
    "elif median_ptratio < 15:\n",
    "    print(f\"   The median ratio of {median_ptratio:.1f} suggests good educational resources.\")\n",
    "else:\n",
    "    print(f\"   The median ratio of {median_ptratio:.1f} is in a moderate range.\")\n",
    "\n",
    "print(f\"   Lower ratios generally indicate better educational resources per student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pupil-teacher ratio distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(Boston['ptratio'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(median_ptratio, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Median: {median_ptratio:.2f}')\n",
    "plt.axvline(mean_ptratio, color='orange', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {mean_ptratio:.2f}')\n",
    "plt.xlabel('Pupil-Teacher Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Pupil-Teacher Ratios')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(Boston['ptratio'], patch_artist=True, \n",
    "            boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
    "plt.ylabel('Pupil-Teacher Ratio')\n",
    "plt.title('Box Plot of Pupil-Teacher Ratios')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h): Lowest Home Value Analysis\n",
    "\n",
    "Let's identify the suburb with the lowest median home value and analyze its characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find suburb with lowest median home value\n",
    "print(\"LOWEST HOME VALUE ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "lowest_value_idx = Boston['medv'].idxmin()\n",
    "lowest_value_suburb = Boston.loc[lowest_value_idx]\n",
    "lowest_value = lowest_value_suburb['medv']\n",
    "\n",
    "print(f\"\\nüè† SUBURB WITH LOWEST MEDIAN HOME VALUE:\")\n",
    "print(f\"   Index: {lowest_value_idx}\")\n",
    "print(f\"   Lowest median home value: ${lowest_value:.1f}k\")\n",
    "print(f\"   Overall median home value: ${Boston['medv'].median():.1f}k\")\n",
    "print(f\"   This is {((Boston['medv'].median() - lowest_value) / lowest_value * 100):.0f}% below the overall median\")\n",
    "\n",
    "print(f\"\\nüìä CHARACTERISTICS OF THIS SUBURB:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# Compare this suburb's characteristics to overall distributions\n",
    "for col in Boston.columns:\n",
    "    if col != 'medv':  # Skip the target variable\n",
    "        suburb_value = lowest_value_suburb[col]\n",
    "        percentile = (Boston[col] <= suburb_value).mean() * 100\n",
    "        \n",
    "        # Determine if this is high or low\n",
    "        if percentile >= 90:\n",
    "            status = \"üî¥ VERY HIGH\"\n",
    "        elif percentile >= 75:\n",
    "            status = \"üü† HIGH\"\n",
    "        elif percentile <= 10:\n",
    "            status = \"üîµ VERY LOW\"\n",
    "        elif percentile <= 25:\n",
    "            status = \"üü° LOW\"\n",
    "        else:\n",
    "            status = \"‚ö™ MODERATE\"\n",
    "        \n",
    "        print(f\"{col.upper():8} : {suburb_value:7.3f} ({percentile:5.1f}th percentile) {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed comparison visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle(f'Characteristics of Lowest Value Suburb (${lowest_value:.1f}k) vs All Suburbs', fontsize=16)\n",
    "\n",
    "# Select key variables for comparison\n",
    "key_vars = ['crim', 'indus', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat']\n",
    "\n",
    "for i, var in enumerate(key_vars):\n",
    "    row, col = i // 3, i % 3\n",
    "    \n",
    "    # Create histogram of all values\n",
    "    axes[row, col].hist(Boston[var], bins=30, alpha=0.7, color='lightblue', \n",
    "                       edgecolor='black', label='All Suburbs')\n",
    "    \n",
    "    # Mark the lowest value suburb\n",
    "    suburb_val = lowest_value_suburb[var]\n",
    "    axes[row, col].axvline(suburb_val, color='red', linestyle='-', linewidth=3, \n",
    "                          label=f'Lowest Value Suburb: {suburb_val:.2f}')\n",
    "    \n",
    "    # Mark median\n",
    "    median_val = Boston[var].median()\n",
    "    axes[row, col].axvline(median_val, color='green', linestyle='--', linewidth=2, \n",
    "                          label=f'Median: {median_val:.2f}')\n",
    "    \n",
    "    axes[row, col].set_xlabel(var.upper())\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].legend(fontsize=8)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary interpretation\n",
    "print(\"\\nüí° INTERPRETATION OF LOWEST VALUE SUBURB:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThis suburb represents a severely disadvantaged area with multiple challenges:\")\n",
    "print(\"\\nüö® MAJOR ISSUES:\")\n",
    "print(\"   ‚Ä¢ Crime rate in the top 1% (extremely dangerous area)\")\n",
    "print(\"   ‚Ä¢ Very high industrial proportion (poor residential environment)\")\n",
    "print(\"   ‚Ä¢ High pollution levels (poor air quality)\")\n",
    "print(\"   ‚Ä¢ Poor pupil-teacher ratio (underfunded schools)\")\n",
    "print(\"   ‚Ä¢ High percentage of lower-status population (economic disadvantage)\")\n",
    "\n",
    "print(\"\\nüèôÔ∏è URBAN CONTEXT:\")\n",
    "print(\"   ‚Ä¢ High accessibility to highways (urban area)\")\n",
    "print(\"   ‚Ä¢ High tax rates (but low property values)\")\n",
    "print(\"   ‚Ä¢ Old housing stock (built before 1940)\")\n",
    "\n",
    "print(\"\\nüìà POLICY IMPLICATIONS:\")\n",
    "print(\"   ‚Ä¢ This area would benefit from comprehensive urban renewal\")\n",
    "print(\"   ‚Ä¢ Investment in education, safety, and environmental cleanup needed\")\n",
    "print(\"   ‚Ä¢ Represents the type of area that requires targeted intervention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Room Count Analysis\n",
    "\n",
    "Let's examine how many suburbs average more than 7 and 8 rooms per dwelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Room count analysis\n",
    "print(\"ROOM COUNT ANALYSIS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Count suburbs with different room thresholds\n",
    "more_than_7_rooms = Boston[Boston['rm'] > 7]\n",
    "more_than_8_rooms = Boston[Boston['rm'] > 8]\n",
    "total_suburbs = len(Boston)\n",
    "\n",
    "print(f\"\\nüèòÔ∏è  HOUSING SIZE DISTRIBUTION:\")\n",
    "print(f\"   Total suburbs: {total_suburbs}\")\n",
    "print(f\"   Suburbs averaging > 7 rooms per dwelling: {len(more_than_7_rooms)} ({len(more_than_7_rooms)/total_suburbs*100:.1f}%)\")\n",
    "print(f\"   Suburbs averaging > 8 rooms per dwelling: {len(more_than_8_rooms)} ({len(more_than_8_rooms)/total_suburbs*100:.1f}%)\")\n",
    "\n",
    "# Room statistics\n",
    "print(f\"\\nüìè ROOM COUNT STATISTICS:\")\n",
    "print(f\"   Mean rooms per dwelling: {Boston['rm'].mean():.2f}\")\n",
    "print(f\"   Median rooms per dwelling: {Boston['rm'].median():.2f}\")\n",
    "print(f\"   Range: {Boston['rm'].min():.1f} to {Boston['rm'].max():.1f} rooms\")\n",
    "print(f\"   Standard deviation: {Boston['rm'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of large home suburbs\n",
    "if len(more_than_8_rooms) > 0:\n",
    "    print(f\"\\nüèõÔ∏è  CHARACTERISTICS OF SUBURBS WITH >8 ROOMS PER DWELLING:\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Compare characteristics\n",
    "    print(f\"\\nüìä COMPARISON: >8 ROOMS vs ALL SUBURBS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    comparison_vars = ['medv', 'crim', 'tax', 'ptratio', 'lstat', 'nox', 'dis']\n",
    "    var_names = ['Home Value ($1000s)', 'Crime Rate', 'Tax Rate', 'Pupil-Teacher Ratio', \n",
    "                'Lower Status %', 'NOX Pollution', 'Distance to Employment']\n",
    "    \n",
    "    for var, name in zip(comparison_vars, var_names):\n",
    "        large_home_mean = more_than_8_rooms[var].mean()\n",
    "        all_suburbs_mean = Boston[var].mean()\n",
    "        difference = ((large_home_mean - all_suburbs_mean) / all_suburbs_mean) * 100\n",
    "        \n",
    "        direction = \"‚Üë\" if difference > 0 else \"‚Üì\"\n",
    "        print(f\"{name:20}: {large_home_mean:7.2f} vs {all_suburbs_mean:7.2f} ({direction}{abs(difference):5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà DETAILED STATISTICS FOR >8 ROOM SUBURBS:\")\n",
    "    print(more_than_8_rooms.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize room count analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Housing Size Analysis', fontsize=16)\n",
    "\n",
    "# Room distribution histogram\n",
    "axes[0, 0].hist(Boston['rm'], bins=25, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "axes[0, 0].axvline(7, color='orange', linestyle='--', linewidth=2, label='7 rooms')\n",
    "axes[0, 0].axvline(8, color='red', linestyle='--', linewidth=2, label='8 rooms')\n",
    "axes[0, 0].axvline(Boston['rm'].mean(), color='green', linestyle='-', linewidth=2, \n",
    "                   label=f'Mean: {Boston['rm'].mean():.2f}')\n",
    "axes[0, 0].set_xlabel('Average Rooms per Dwelling')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Room Counts')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rooms vs Home Value\n",
    "axes[0, 1].scatter(Boston['rm'], Boston['medv'], alpha=0.6, color='blue')\n",
    "axes[0, 1].set_xlabel('Average Rooms per Dwelling')\n",
    "axes[0, 1].set_ylabel('Median Home Value ($1000s)')\n",
    "axes[0, 1].set_title('Rooms vs Home Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_rm_medv = Boston['rm'].corr(Boston['medv'])\n",
    "axes[0, 1].text(0.05, 0.95, f'Correlation: {corr_rm_medv:.3f}', \n",
    "                transform=axes[0, 1].transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Rooms vs Crime Rate (log scale)\n",
    "axes[1, 0].scatter(Boston['rm'], Boston['crim'], alpha=0.6, color='red')\n",
    "axes[1, 0].set_xlabel('Average Rooms per Dwelling')\n",
    "axes[1, 0].set_ylabel('Crime Rate (log scale)')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].set_title('Rooms vs Crime Rate')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_rm_crim = Boston['rm'].corr(Boston['crim'])\n",
    "axes[1, 0].text(0.05, 0.95, f'Correlation: {corr_rm_crim:.3f}', \n",
    "                transform=axes[1, 0].transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Box plot comparison\n",
    "room_categories = ['‚â§7 rooms', '>8 rooms']\n",
    "room_data = [Boston[Boston['rm'] <= 7]['medv'], more_than_8_rooms['medv']]\n",
    "box_plot = axes[1, 1].boxplot(room_data, labels=room_categories, patch_artist=True)\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1, 1].set_ylabel('Median Home Value ($1000s)')\n",
    "axes[1, 1].set_title('Home Values by Room Category')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final interpretation of room analysis\n",
    "print(\"\\nüí° INTERPRETATION OF ROOM COUNT ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüè† HOUSING SIZE PATTERNS:\")\n",
    "print(f\"   ‚Ä¢ Large homes (>8 rooms) are rare luxury items ({len(more_than_8_rooms)/total_suburbs*100:.1f}% of suburbs)\")\n",
    "print(f\"   ‚Ä¢ Most suburbs have 5-7 rooms per dwelling (typical family homes)\")\n",
    "print(f\"   ‚Ä¢ Strong positive correlation between room count and home value (r = {Boston['rm'].corr(Boston['medv']):.3f})\")\n",
    "\n",
    "print(f\"\\nüåü CHARACTERISTICS OF LARGE HOME SUBURBS (>8 rooms):\")\n",
    "if len(more_than_8_rooms) > 0:\n",
    "    print(\"   ‚Ä¢ MUCH higher median home values (affluent areas)\")\n",
    "    print(\"   ‚Ä¢ MUCH lower crime rates (safer neighborhoods)\")\n",
    "    print(\"   ‚Ä¢ Lower tax rates (efficient municipal services)\")\n",
    "    print(\"   ‚Ä¢ Better pupil-teacher ratios (better funded schools)\")\n",
    "    print(\"   ‚Ä¢ Lower percentage of lower-status population (higher socioeconomic status)\")\n",
    "    print(\"   ‚Ä¢ Less pollution (better environmental quality)\")\n",
    "\n",
    "print(f\"\\nüéØ CONCLUSION:\")\n",
    "print(\"   Room count serves as an excellent proxy for affluence and quality of life.\")\n",
    "print(\"   Large homes cluster in advantaged areas with multiple positive characteristics.\")\n",
    "print(\"   This demonstrates how housing characteristics reflect broader socioeconomic patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This comprehensive analysis of the Boston housing dataset has revealed several important insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPREHENSIVE SUMMARY OF BOSTON HOUSING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìã DATASET OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ {Boston.shape[0]} Boston suburbs with {Boston.shape[1]} characteristics each\")\n",
    "print(f\"   ‚Ä¢ Mix of socioeconomic, environmental, and housing variables\")\n",
    "print(f\"   ‚Ä¢ Target: Median home values ranging from ${Boston['medv'].min():.1f}k to ${Boston['medv'].max():.1f}k\")\n",
    "\n",
    "print(f\"\\nüîç KEY FINDINGS:\")\n",
    "print(f\"   ‚Ä¢ Strong socioeconomic stratification across Boston suburbs\")\n",
    "print(f\"   ‚Ä¢ Crime correlates with urban density, accessibility, and industrial activity\")\n",
    "print(f\"   ‚Ä¢ Only {len(charles_river_suburbs)} suburbs ({charles_percentage:.1f}%) have waterfront access\")\n",
    "print(f\"   ‚Ä¢ Educational resources vary significantly (PT ratio: {Boston['ptratio'].min():.1f} to {Boston['ptratio'].max():.1f})\")\n",
    "print(f\"   ‚Ä¢ Housing size strongly predicts affluence and quality of life\")\n",
    "\n",
    "print(f\"\\nüèòÔ∏è NEIGHBORHOOD TYPES IDENTIFIED:\")\n",
    "print(f\"   ‚Ä¢ Disadvantaged areas: High crime, pollution, poor schools, low home values\")\n",
    "print(f\"   ‚Ä¢ Affluent suburbs: Large homes, low crime, good schools, high values\")\n",
    "print(f\"   ‚Ä¢ Waterfront premium: Charles River access commands higher values\")\n",
    "print(f\"   ‚Ä¢ Industrial zones: High pollution, crime, but accessible to employment\")\n",
    "\n",
    "print(f\"\\nüìä STATISTICAL INSIGHTS:\")\n",
    "print(f\"   ‚Ä¢ Multicollinearity exists among several predictors\")\n",
    "print(f\"   ‚Ä¢ Strong correlations between urban characteristics (RAD-TAX: {Boston['rad'].corr(Boston['tax']):.3f})\")\n",
    "print(f\"   ‚Ä¢ Environmental and socioeconomic factors are interconnected\")\n",
    "print(f\"   ‚Ä¢ Housing characteristics reflect broader community attributes\")\n",
    "\n",
    "print(f\"\\nüéØ DATA MINING APPLICATIONS:\")\n",
    "print(f\"   ‚Ä¢ Urban planning: Identify areas needing investment\")\n",
    "print(f\"   ‚Ä¢ Real estate: Predictive modeling for property values\")\n",
    "print(f\"   ‚Ä¢ Policy analysis: Target interventions for disadvantaged areas\")\n",
    "print(f\"   ‚Ä¢ Machine learning: Feature engineering and model selection guidance\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS COMPLETE - GROUP 1, AUCA DATA MINING\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
