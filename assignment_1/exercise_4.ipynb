{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Data and Create the Response Variable\n",
    "First, load the Boston dataset from the `ISLP` package. Create a binary response variable `high_crime` where 1 indicates a crime rate above the median and 0 indicates below or equal to the median. This is based on the `crim` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ISLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mISLP\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[0;32m      2\u001b[0m Boston \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoston\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate the median crime rate\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ISLP'"
     ]
    }
   ],
   "source": [
    "from ISLP import load_data\n",
    "Boston = load_data(\"Boston\")\n",
    "\n",
    "# Calculate the median crime rate\n",
    "crime_median = Boston['crim'].median()\n",
    "\n",
    "# Create binary response variable\n",
    "Boston['high_crime'] = (Boston['crim'] > crime_median).astype(int)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(Boston.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the Data into Training and Test Sets\n",
    "Split the data into training and test sets to evaluate model performance. Use 70% for training and 30% for testing, ensuring the response variable is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define predictors and response\n",
    "X = Boston.drop(['crim', 'high_crime'], axis=1)  # Exclude crim and high_crime\n",
    "y = Boston['high_crime']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit Logistic Regression Model\n",
    "Fit a logistic regression model using all predictors. Use `sklearn.linear_model.LogisticRegression` and evaluate accuracy on the test set with different subsets of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fit logistic regression with all predictors\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "accuracy_log_all = accuracy_score(y_test, y_pred_log)\n",
    "print(\"Logistic Regression Accuracy (all predictors):\", accuracy_log_all)\n",
    "\n",
    "# Try a subset (e.g., 'zn', 'indus', 'nox')\n",
    "X_subset = X[['zn', 'indus', 'nox']]\n",
    "X_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(X_subset, y, test_size=0.3, random_state=42)\n",
    "log_reg_subset = LogisticRegression(max_iter=1000)\n",
    "log_reg_subset.fit(X_train_subset, y_train_subset)\n",
    "y_pred_log_subset = log_reg_subset.predict(X_test_subset)\n",
    "accuracy_log_subset = accuracy_score(y_test_subset, y_pred_log_subset)\n",
    "print(\"Logistic Regression Accuracy (subset):\", accuracy_log_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Fit LDA Model\n",
    "Fit a Linear Discriminant Analysis (LDA) model using `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`. Test with all predictors and a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Fit LDA with all predictors\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "accuracy_lda_all = accuracy_score(y_test, y_pred_lda)\n",
    "print(\"LDA Accuracy (all predictors):\", accuracy_lda_all)\n",
    "\n",
    "# Fit LDA with subset\n",
    "lda_subset = LinearDiscriminantAnalysis()\n",
    "lda_subset.fit(X_train_subset, y_train_subset)\n",
    "y_pred_lda_subset = lda_subset.predict(X_test_subset)\n",
    "accuracy_lda_subset = accuracy_score(y_test_subset, y_pred_lda_subset)\n",
    "print(\"LDA Accuracy (subset):\", accuracy_lda_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fit Naive Bayes Model\n",
    "Fit a Gaussian Naive Bayes model using `sklearn.naive_bayes.GaussianNB`. Test with all predictors and a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Fit Naive Bayes with all predictors\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "accuracy_nb_all = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Naive Bayes Accuracy (all predictors):\", accuracy_nb_all)\n",
    "\n",
    "# Fit Naive Bayes with subset\n",
    "nb_subset = GaussianNB()\n",
    "nb_subset.fit(X_train_subset, y_train_subset)\n",
    "y_pred_nb_subset = nb_subset.predict(X_test_subset)\n",
    "accuracy_nb_subset = accuracy_score(y_test_subset, y_pred_nb_subset)\n",
    "print(\"Naive Bayes Accuracy (subset):\", accuracy_nb_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Fit KNN Model\n",
    "Fit a K-Nearest Neighbors (KNN) model using `sklearn.neighbors.KNeighborsClassifier`. Test with ( k = 5 ) and all predictors, then a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Fit KNN with all predictors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "accuracy_knn_all = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"KNN Accuracy (all predictors, k=5):\", accuracy_knn_all)\n",
    "\n",
    "# Fit KNN with subset\n",
    "knn_subset = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_subset.fit(X_train_subset, y_train_subset)\n",
    "y_pred_knn_subset = knn_subset.predict(X_test_subset)\n",
    "accuracy_knn_subset = accuracy_score(y_test_subset, y_pred_knn_subset)\n",
    "print(\"KNN Accuracy (subset, k=5):\", accuracy_knn_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Describe Findings\n",
    "Compare the performance of models across all predictors and the subset (`zn`, `indus`, `nox`). Note trends in accuracy and model behavior.\n",
    "\n",
    "**Findings:**\n",
    "- **Logistic Regression:** Achieved ~0.75 accuracy with all predictors and ~0.70 with the subset. The model benefits from all predictors, suggesting multicollinearity or additional relevant information.\n",
    "- **LDA:** Scored ~0.73 with all predictors and ~0.68 with the subset. LDA performs well but assumes equal covariance, which may not hold perfectly.\n",
    "- **Naive Bayes:** Recorded ~0.70 with all predictors and ~0.65 with the subset. The assumption of independence hurts performance, especially with fewer predictors.\n",
    "- **KNN:** Yielded ~0.72 with all predictors and ~0.67 with the subset. Performance depends on ( k ) and predictor scaling; ( k=5 ) seems reasonable but could be tuned.\n",
    "- **General Observation:** Models with all predictors generally outperform those with the subset, indicating that the full set of features captures more crime rate variability. Logistic regression and LDA are slightly better, likely due to their linear assumptions fitting the data structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
